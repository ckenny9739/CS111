Connor Kenny and Justin Liu
CS 111 - Lab 4

PART 1:

We defined fairly consitently failing as happening more than 60% of the time when run over 10 times.
For each thread number here is the number of iterations it took us to fairly consistently fail:
1: N/A bc never failed
2: ~1500-2000
3: ~1100 
4: ~900
5: ~800
6: ~700
7: ~600
8: ~500
9: ~450
10:~400 


1.1
1) It takes so iterations because as you add iterations, there are more chances for the race condition to occur. With a small number of iterations, the odds are less. It takes more threads to make it fail for a fewer number of iterations because with more threads there are more chances that two threads will try to access count at the same time. This increases the chances of the race condition making count incorrect. Also, we probably have a lot more failures than are printed because even if the counter ends up at 0, that doesn't mean that the race conditions never messed up count. It just means that it also happened to fix it on the other side (add only 1 instead of 2, then only subtract 1 instead of 2). Basically, when the number of threads or number of iterations is low there is a very small chance that the race condition will mess up the program because it will run so fast. For the program to result in an error, we need a lot of threads to have the chance to call add at the same time and/or have a lot of iterations so we have a bigger chance to fail.

2) Small number of iterations do not fail very often because there is basically no time for the race condition to occur in the add function. We found that the race condition error happens when the number of threads or iterations is high because that increases the odds of two threads being inside the add function at the same time, which can result in both accessing the pointer and then increasing it or decreasing it incorrectly. Basically, for small number of iterations, there are less chances for the race condition to ruin our results.

Number of iterations needed for a failure (with yield):
1: N/A bc never failed
2: ~20
3: ~15
4: ~13
5: ~10
6: ~7
7: ~6
8: ~6
9: ~6
10:~6

The number of iterations needed basically dropped by a factor of 100!!!

Comparison of times for regular and yield:
# Threads     # Iterations	Regular(ns)	Yield(ns)
1 	      1			220K		268K
1	      100		233K		347K
1	      10000		433K		924K

2	      1			210K		218K
2	      100		220K		328K
2	      10000		1.29M		11.4M

3	      1			217K		226K
3             100		219K		328K
3             10000		2.21M		10.5M

4	      1			270K		296K
4	      100		269K		374K
4	      10000		2.88M		10.5M

5	      1			304K		302K
5	      100		313K		413K
5	      10000		3.44M		10.7M

6	      1			327K		362K
6	      100		328K		501K
6	      10000		4.46M		10.9M

7	      1			320K		330K
7	      100		341K		455K
7	      10000		5.57M		13.9M

8	      1			360K		367K
8	      100		378K		797K
8	      10000		6.7M		14.4M
	      
9	      1			397K		424K
9	      100		412K		519K
9	      10000		7.17M		16.1M

10	      1			435K		760K
10	      100		467K		551K
10	      10000		7.82M		17.6M


Obviously, the ones with yield take much longer than the ones that don't. We found it to be roughly 1.5 to 10 faster depending on number of threads. As more threads are added, it goes from roughly 10x faster (2 threads) for Regular and goes to roughly 2x faster (10 threads) for Regular.

(Graph is in file nonyield.pdf) We graphed the average cost per operation by taking the time per operation as a function of number of iterations. We could only go to 100 iterations because the scale of the graph got too out of hand. The graph shows that the cost per operation goes from roughly 100K to almost 0. If we were to graph it over all iterations from 0 to infinity, it would definitely tend toward the actual cost to perform the action. For some reference for bigger iteration numbers, we also calculated these values:
(number of iterations)	    (time per op in ns)
1000	   		    134
5000			    29
10000			    15
50000			    10
100000			    9

As you can see, the number of operations goes down exponentially as the number
of iterations increases and tends towards a number that is actually lower than 10 ns per operation.

1.2
1) The average cost per iteration drops with increasing iterations because the overhead is a larger percentatge of overall time when the number of threads/iterations is small. As these numbers increase, more time is spent on the threads' actual work instead of setting them up.

2) The correct cost would be without taking into account any overhead. This could be found by taking the limit of the cost per operation as the number of iterations goes to infinity. It will tend towards a single value which is the correct cost. 

3) The yield runs are much slower because not every time through the loop do we actually add. Sometimes, the yield function will be called and then the other threads will come in and cause the race condition. This basically makes sure that it is likely for the race condition error to happen. Overall, the extra time is going into yielding instead of actually performing useful operations.

4) We can never get exactly valid timings because we added an extra conditional statement. With this statement, even at only 1 thread who cannot yield to any others, it has to go through the check which will actually affect the timings. We can get nearly perfect times with only one thread because it basically never yields. As we add threads, it becomes more and more of a time sink because each thread has to yield before it can actually perform an operation. If we could somehow account for these times perfectly, then we could subtract the yield time from the operational time to get the real times. 


1.3
1) They perform similarly because there are not many computations for the lock so there is a very low chance of actually needing to use it. You don't have to waste a lot of time trying to get the lock. When you have 20 threads, only 1 gets it at a time so 19 wait. When there are only 2, only 1 has to wait. When there is only 1, none ever have to wait.

2) The options slow down because they require the add to only be called when the thread has the lock. This makes it so that as you add threads, more are waiting and wasting time. This makes it more inefficient, especially as threads increases. It is important however if we want to avoid the race conditions and get the correct answer of 0.

3) Spin locks are very expensive because many threads get stuck spinning. It has to do with the wait loop which takes a lot of time and renders a thread useless. With a lot of threads, it will make a lot of threads waste time which is very costly. The more threads that you have, the more time that a (n-1) / n threads will be wasting cpu time spinning.

We graphed the data on Excel and put them in the following files: unprotected.pdf, mutex.pdf, spinlock.pdf and comswap.pdf for unprotected, mutext, spin lock and compare-and-swap.  

------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------

2.1
As number of iterations increases, so does the time per operation. This means that the time per operation is proportional to the

The time to operate is proportional to the length of the list

divide the time by the list size? Not always right

Since we insert at the beginning, when we do lookup (if done in same order, each is proporitonal to lenght of list). If we reverse the order of where we look (we can get it in O(1))

We can get it to be O(1), but if we shuffle the list then it is proportional


2.2

2.3

------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------

3.1

1) The mutex must be held before you can call pthread_cond_wait because if you do not do this, a thread can miss its wakeup call. Basically, if a thread acquires the mutext first, then is waiting for the condition to become true and another thread makes the condition true and can beat the first thread to action. This makes the original thread who has been waiting for the condition to become true miss its chance to wake up. This problem is solved if both have to acquire the mutex before being able to call pthread_cond_wait. Overall, each thread must acquire the mutex or else there is an unavoidable race condition of another thread coming in and messing with the shared data when the original thread should have had a chance to respond.

2) The mutex must be released so that other threads who are waiting on the condition or those threads that can actually change the condition to true have a chance to work with the shared data. If the thread never gives up the shared mutex, no other threads could ever go in and change the data to let the original thread know that it is okay to move on because the original predicate condition is now true. Overall, this is the only way that the original thread can repeatedly check for a change in the predicate condition value reliably and for the other threads to have a chance at changing the shared memory.

3) The mutex must be reacquired when the calling thread resumes because it needs to make sure that the shared data does not get changed any further from what made the condition variable true. If it were not to acquire the mutex and just wake up randomly, there is a chance that another thread could come in and make the condition false again. This would leave the original thread in a very awkward position with really no guidance of what to do. Overall, the mutex must be reacquired so that the resuming thread knows exactly which state it is returning back into so that it can do its job accordingly.

4) This all must be done inside pthread_cond_wait because it does these operations atomically. If the user were to just unlock the mutex on his own and then wait for the condition to become true before waking back up, there would be race conditions in between unlocking the mutex manually and checking the condition variable. By putting it all in a single system call, we are able to be sure that the check is done properly as long as the threads follow the designated preconditions for the system call. The idea is that pthread_cond_wait is waiting for the certain condition in shared memory to become true. This is a very dangerous place if synchronization is not utilized so the pthread library allows programmers to utilize this system call to reliably do the work without risking extra race conditions.

5) The bulk of the work that pthread_cond_wait does is to check the condition and sleep if the condition has not been met yet. This needs to be done atomically as there is a chance that another thread could come in and make the condition true after the the thread checked the condition but before it went to sleep. This is why the thread must control the mutex before it can call the function. This way no other threads can sneak in an mess with the shared memory. In order to do this entirely in user mode, the mutex must be acquired before checking and must be atomically released upon sleeping. It is possible to do this, but it does require that the critical section be perfectly atomic. If it is not, then there is no way to ensure that anything is done correctly and pthread_cond_wait would be useless. 
